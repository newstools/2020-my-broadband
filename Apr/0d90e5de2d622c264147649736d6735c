Researchers from Facebook have released a paper detailing a new “Web Enabled Simulation” (WES) of the company’s social media platform which is populated entirely by bots. The simulation, named WW, aims to help Facebook learn about the patterns that describe abusive behaviour so that it can more effectively police harassment, abuse, and scams on its actual Facebook platform. This AI-populated version of Facebook will include bots which are programmed to act maliciously, targeting other users in a similar way that scammers or bad actors on the real Facebook would. Facebook is expanding on an earlier simulation which it ran to observe behaviour trends named Sapienz, but the researchers said that this new simulation is far more similar to the real Facebook than any other before it. “Unlike traditional simulation, in which a model of reality is created, a WES system is thus built on a real-world software platform,” the researchers said. “In order to model users’ behaviour on a WES system, a multi-agent-based approach is used, in which each agent is essentially a bot that simulates user behaviour.” “This user behaviour could be captured in a rule-based system or could be learnt, either supervised from examples or unsupervised in a reinforcement learning setting,” researchers said. This simulation of the Facebook platform will be completely closed off to human users and will be run internally to provide information on how Facebook can improve its infrastructure which prevents and punishes bad actors for breaking community standards. The findings of the simulation’s processing could have widespread uses for the Facebook social media platform, the researchers said. “Community behaviour is increasingly prevalent in software applications, for example for travel, accommodation, entertainment, and shopping.” “These systems use social interactions so that each user can benefit from the collective experience of other users,” the researchers said. “Although this paper focuses on Facebook’s WW system, the concepts and approach could also find application in platforms used by other organisations.” One example of bad behaviour on Facebook which may benefit from the results of the simulation is the spread of fake news by bad actors. Facebook, Google, Microsoft, and other companies joined forces last month to combat the spread of fake news across the world’s biggest social networks. Specifically, the world’s largest social media platforms are working together to crack down on the circulation of misinformation about the COVID-19 coronavirus. Facebook also recently announced that it would run free adverts about the coronavirus to combat fake news in partnership with the World Health Organisation. “It’s important that everyone has a place to share their experiences and talk about the outbreak, but as our community standards make clear, it’s not okay to share something that puts people in danger,” said Facebook CEO Mark Zuckerberg. He said that Facebook will remove false claims and conspiracy theories that have been flagged by leading global health organisations.